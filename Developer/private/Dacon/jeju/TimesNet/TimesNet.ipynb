{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "## DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import warnings\n",
    "from utils import data_split\n",
    "\n",
    "# 모든 경고 메시지를 무시하고 출력하지 않음\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "## Model\n",
    "import torch    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "from layers.Embed import DataEmbedding\n",
    "from layers.Conv_Blocks import Inception_Block_V1   \n",
    "            #convolution block used for convoluting the 2D time data, changeable\n",
    "\n",
    "## Train\n",
    "import time\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train + test(4년간 3월 평균)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_code = ['BC_C_J', 'TG_B_J', 'CR_B_J', 'RD_E_S', 'BC_A_J', 'CB_F_J', 'RD_D_J', 'TG_A_S', 'BC_E_S', 'CR_D_J', 'BC_A_S', 'BC_B_S', 'TG_E_J', \n",
    "               'CR_E_S', 'RD_F_J', 'BC_E_J', 'TG_A_J', 'CR_C_J', 'CR_D_S', 'TG_C_J', 'CB_A_S', 'TG_D_J', 'CR_E_J', 'RD_C_S', 'BC_C_S', 'CB_E_J', \n",
    "               'RD_E_J', 'BC_D_J', 'CR_A_J', 'TG_E_S', 'TG_C_S', 'TG_D_S', 'RD_A_S', 'RD_A_J', 'RD_D_S', 'TG_B_S', 'CB_D_J', 'CB_A_J', 'BC_B_J']\n",
    "\n",
    "data = pd.read_csv('~/Developer/private/Dacon/jeju/data/train.csv')\n",
    "data_list = data_split(data)\n",
    "\n",
    "dataset = {}\n",
    "march_data = {}\n",
    "for code in unique_code:\n",
    "    march_data[code] = {}\n",
    "    march_data[code]['2019'] = data_list[f'data_{code}'][(data_list[f'data_{code}']['timestamp'] >= '2019-03-04') & (data_list[f'data_{code}']['timestamp'] <= '2019-03-31')].reset_index(drop = True)\n",
    "    march_data[code]['2020'] = data_list[f'data_{code}'][(data_list[f'data_{code}']['timestamp'] >= '2020-03-04') & (data_list[f'data_{code}']['timestamp'] <= '2020-03-31')].reset_index(drop = True)\n",
    "    march_data[code]['2021'] = data_list[f'data_{code}'][(data_list[f'data_{code}']['timestamp'] >= '2021-03-04') & (data_list[f'data_{code}']['timestamp'] <= '2021-03-31')].reset_index(drop = True)\n",
    "    march_data[code]['2022'] = data_list[f'data_{code}'][(data_list[f'data_{code}']['timestamp'] >= '2022-03-04') & (data_list[f'data_{code}']['timestamp'] <= '2022-03-31')].reset_index(drop = True)\n",
    "    \n",
    "\n",
    "    avg_supply = []\n",
    "    for i in range(28):\n",
    "        supply_19 = march_data[code]['2019']['supply(kg)'][i]\n",
    "        supply_20 = march_data[code]['2020']['supply(kg)'][i] \n",
    "        supply_21 = march_data[code]['2021']['supply(kg)'][i] \n",
    "        supply_22 =march_data[code]['2022']['supply(kg)'][i] \n",
    "\n",
    "        supplies = [supply_19, supply_20, supply_21, supply_22]\n",
    "        filtered_supplies = [supply for supply in supplies if supply != 0]\n",
    "\n",
    "        # 평균 계산\n",
    "        if filtered_supplies:\n",
    "            average_supply = sum(filtered_supplies) / len(filtered_supplies)\n",
    "        else: \n",
    "            average_supply = 0\n",
    "        \n",
    "        avg_supply.append(average_supply)\n",
    "\n",
    "    zero_sunday = [1, 8, 15, 22]\n",
    "    for idx in zero_sunday:\n",
    "        avg_supply[idx] = 0\n",
    "\n",
    "\n",
    "    avg_price = []\n",
    "    for i in range(28):\n",
    "        price_19 = march_data[code]['2019']['price(원/kg)'][i]\n",
    "        price_20 = march_data[code]['2020']['price(원/kg)'][i] \n",
    "        price_21 = march_data[code]['2021']['price(원/kg)'][i] \n",
    "        price_22 =march_data[code]['2022']['price(원/kg)'][i] \n",
    "\n",
    "        prices = [price_19, price_20, price_21, price_22]\n",
    "        filtered_prices = [price for price in prices if price != 0]\n",
    "\n",
    "        # 평균 계산\n",
    "        if filtered_prices:\n",
    "            average_price = sum(filtered_prices) / len(filtered_prices)\n",
    "        else: \n",
    "            average_price = 0\n",
    "        \n",
    "        avg_price.append(average_price)\n",
    "        \n",
    "\n",
    "    zero_sunday = [1, 8, 15, 22]\n",
    "    for idx in zero_sunday:\n",
    "        avg_price[idx] = 0\n",
    "\n",
    "\n",
    "    march_data[code]['avg_supply'] = avg_supply\n",
    "    march_data[code]['avg_price'] = avg_price\n",
    "\n",
    "    # 2023년 3월 test용 데이터셋 생성\n",
    "    new_df = march_data[code]['2019'].copy()\n",
    "    # 'ID'의 연도를 2023으로 변경\n",
    "    new_df['ID'] = new_df['ID'].apply(lambda x: f\"{x[:5]}2023{x[9:]}\")\n",
    "    # 'timestamp'의 연도를 2023으로 변경\n",
    "    new_df['timestamp'] = new_df['timestamp'].apply(lambda x: f\"2023-{x[5:]}\")\n",
    "    # supply와 price의 평균값으로 변경\n",
    "    new_df['supply(kg)'] = avg_supply\n",
    "    new_df['price(원/kg)'] = avg_price\n",
    "\n",
    "    march_data[code]['2023'] = new_df\n",
    "    \n",
    "\n",
    "    # 원래 데이터와 합침\n",
    "    merged_df = pd.concat([data_list[f'data_{code}'], march_data[code]['2023']], axis=0, ignore_index=True)\n",
    "    dataset[code] = merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>item</th>\n",
       "      <th>corporation</th>\n",
       "      <th>location</th>\n",
       "      <th>supply(kg)</th>\n",
       "      <th>price(원/kg)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TG_A_S_20190101</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TG_A_S_20190102</td>\n",
       "      <td>2019-01-02</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TG_A_S_20190103</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>190591.000000</td>\n",
       "      <td>2526.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TG_A_S_20190104</td>\n",
       "      <td>2019-01-04</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>137729.000000</td>\n",
       "      <td>2134.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TG_A_S_20190105</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>134039.000000</td>\n",
       "      <td>2075.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1546</th>\n",
       "      <td>TG_A_2023190327</td>\n",
       "      <td>2023-03-27</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>9511.000000</td>\n",
       "      <td>5015.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1547</th>\n",
       "      <td>TG_A_2023190328</td>\n",
       "      <td>2023-03-28</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>16388.000000</td>\n",
       "      <td>5254.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1548</th>\n",
       "      <td>TG_A_2023190329</td>\n",
       "      <td>2023-03-29</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>16599.666667</td>\n",
       "      <td>4729.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>TG_A_2023190330</td>\n",
       "      <td>2023-03-30</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>12938.125000</td>\n",
       "      <td>4760.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1550</th>\n",
       "      <td>TG_A_2023190331</td>\n",
       "      <td>2023-03-31</td>\n",
       "      <td>TG</td>\n",
       "      <td>A</td>\n",
       "      <td>S</td>\n",
       "      <td>10446.666667</td>\n",
       "      <td>5306.666667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1551 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID   timestamp item corporation location     supply(kg)  \\\n",
       "0     TG_A_S_20190101  2019-01-01   TG           A        S       0.000000   \n",
       "1     TG_A_S_20190102  2019-01-02   TG           A        S       0.000000   \n",
       "2     TG_A_S_20190103  2019-01-03   TG           A        S  190591.000000   \n",
       "3     TG_A_S_20190104  2019-01-04   TG           A        S  137729.000000   \n",
       "4     TG_A_S_20190105  2019-01-05   TG           A        S  134039.000000   \n",
       "...               ...         ...  ...         ...      ...            ...   \n",
       "1546  TG_A_2023190327  2023-03-27   TG           A        S    9511.000000   \n",
       "1547  TG_A_2023190328  2023-03-28   TG           A        S   16388.000000   \n",
       "1548  TG_A_2023190329  2023-03-29   TG           A        S   16599.666667   \n",
       "1549  TG_A_2023190330  2023-03-30   TG           A        S   12938.125000   \n",
       "1550  TG_A_2023190331  2023-03-31   TG           A        S   10446.666667   \n",
       "\n",
       "      price(원/kg)  \n",
       "0        0.000000  \n",
       "1        0.000000  \n",
       "2     2526.000000  \n",
       "3     2134.000000  \n",
       "4     2075.000000  \n",
       "...           ...  \n",
       "1546  5015.666667  \n",
       "1547  5254.333333  \n",
       "1548  4729.000000  \n",
       "1549  4760.250000  \n",
       "1550  5306.666667  \n",
       "\n",
       "[1551 rows x 7 columns]"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['TG_A_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 64\n",
    "label_len = 28\n",
    "pred_len = 28\n",
    "\n",
    "\n",
    "\n",
    "type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "flag = 'train'\n",
    "set_type = type_map[flag]\n",
    "features = 'S' # single or multi\n",
    "target = 'price(원/kg)' \n",
    "scale = True\n",
    "timeenc = 0 # time_feature가 존재하는지 여부 : 없으면 임의로 생성해주지만 우리 데이터에는 존재함\n",
    "freq = 'd'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset_jeju(Dataset):\n",
    "        def __init__(self, flag='train', size=[64,28,28],\n",
    "             features='S', scale=True, timeenc=0, freq='d', seasonal_patterns=None):\n",
    "            # size [seq_len, label_len, pred_len]\n",
    "            # info\n",
    "            if size == None:\n",
    "                self.seq_len = 24 * 4 * 4 # 16일 데이터를 이용해 \n",
    "                self.label_len = 24 * 4 \n",
    "                self.pred_len = 24 * 4 # 4일간의 데이터 예측? 28\n",
    "            else:\n",
    "                self.seq_len = size[0]\n",
    "                self.label_len = size[1]\n",
    "                self.pred_len = size[2]\n",
    "            # init\n",
    "            assert flag in ['train', 'test', 'val']\n",
    "            type_map = {'train': 0, 'val': 1, 'test': 2}\n",
    "            self.set_type = type_map[flag]\n",
    "            self.features = features\n",
    "            self.target = target\n",
    "            self.scale = scale\n",
    "            self.timeenc = timeenc\n",
    "            self.freq = freq\n",
    "\n",
    "            self.train_mean = None\n",
    "            self.train_std = None\n",
    "\n",
    "            # After initialization, call __read_data__() to manage the data file.\n",
    "            self.__read_data__()\n",
    "\n",
    "        def __read_data__(self):\n",
    "                self.scaler = StandardScaler()\n",
    "\n",
    "                #get raw data from path\n",
    "                df_raw = dataset['TG_B_J']\n",
    "\n",
    "                # split data set into train, vali, test. border1 is the left border and border2 is the right.\n",
    "                # Once flag(train, vali, test) is determined, __read_data__ will return certain part of the dataset.\n",
    "                border1s = [0, 1350 - seq_len, 1495 - seq_len]\n",
    "                border2s = [1350, 1495, 1523]\n",
    "                border1 = border1s[self.set_type]\n",
    "                border2 = border2s[self.set_type]\n",
    "\n",
    "                #decide which columns to select\n",
    "                if self.features == 'M' or self.features == 'MS':\n",
    "                        cols_data = df_raw.columns[1:] # column name list (remove 'date')\n",
    "                        df_data = df_raw[cols_data]  #remove the first column, which is time stamp info\n",
    "                elif self.features == 'S':\n",
    "                        df_data = df_raw[[self.target]] # target column\n",
    "\n",
    "                #scale data by the scaler that fits training data\n",
    "                if self.scale:\n",
    "                        train_data = df_data[border1s[0]:border2s[0]]\n",
    "                        #train_data.values: turn pandas DataFrame into 2D numpy\n",
    "                        self.train_mean = train_data.mean().values\n",
    "                        self.train_std = train_data.std().values\n",
    "                        self.scaler.fit(train_data.values)  \n",
    "                        data = self.scaler.transform(df_data.values)\n",
    "                else:\n",
    "                        data = df_data.values \n",
    "                \n",
    "                # 날짜를 년/월/일/요일 형태로 자르고 리스트로 변환\n",
    "                # [[2019    1    1    1]\n",
    "                #  [2019    1    2    2]\n",
    "                #  [2019    1    3    3]\n",
    "                #  ...\n",
    "                \n",
    "                df_stamp = df_raw[['timestamp']][border1:border2]\n",
    "                df_stamp['timestamp'] = pd.to_datetime(df_stamp.timestamp) \n",
    "\n",
    "                if self.timeenc == 0:  #time feature encoding is fixed or learned\n",
    "                        # df_stamp['year'] = df_stamp.timestamp.apply(lambda row: row.year, 1)\n",
    "                        df_stamp['month'] = df_stamp.timestamp.apply(lambda row: row.month, 1)\n",
    "                        df_stamp['day'] = df_stamp.timestamp.apply(lambda row: row.day, 1)\n",
    "                        df_stamp['weekday'] = df_stamp.timestamp.apply(lambda row: row.weekday(), 1)\n",
    "                        \n",
    "                        #now df_frame has multiple columns recording the month, day etc. time stamp\n",
    "                        # next we delete the 'date' column and turn 'DataFrame' to a list\n",
    "                        data_stamp = df_stamp.drop(['timestamp'], axis = 1).values\n",
    "\n",
    "                # elif self.timeenc == 1: #time feature encoding is timeF\n",
    "                #         data_stamp = time_features(pd.to_datetime(df_stamp['date'].values), freq=self.freq)\n",
    "                #         data_stamp = data_stamp.transpose(1, 0)\n",
    "                        \n",
    "                \n",
    "                # data_x and data_y are same copy of a certain part of data\n",
    "                self.data_x = data[border1:border2]\n",
    "                self.data_y = data[border1:border2]\n",
    "                self.data_stamp = data_stamp\n",
    "\n",
    "        def __getitem__(self, index):\n",
    "                #given an index, calculate the positions after this index to truncate the dataset\n",
    "                s_begin = index\n",
    "                s_end = s_begin + self.seq_len\n",
    "                r_begin = s_end - self.label_len\n",
    "                r_end = r_begin + self.label_len + self.pred_len\n",
    "\n",
    "                #input and output sequence\n",
    "                seq_x = self.data_x[s_begin:s_end]\n",
    "                seq_y = self.data_y[r_begin:r_end]\n",
    "\n",
    "                #time mark\n",
    "                seq_x_mark = self.data_stamp[s_begin:s_end]\n",
    "                seq_y_mark = self.data_stamp[r_begin:r_end]\n",
    "\n",
    "                return seq_x, seq_y, seq_x_mark, seq_y_mark, self.train_mean, self.train_std\n",
    "\n",
    "        def __len__(self):\n",
    "                return len(self.data_x) - self.seq_len - self.pred_len + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 데이터셋 객체 생성\n",
    "dataset_train = Dataset_jeju(flag='train', size=[64, 28, 28], features='S', scale=True, timeenc=0, freq='h')\n",
    "dataset_val = Dataset_jeju(flag='val', size=[64, 28, 28], features='S', scale=True, timeenc=0, freq='h')\n",
    "dataset_test = Dataset_jeju(flag='test', size=[64, 28, 28], features='S', scale=True, timeenc=0, freq='h')\n",
    "\n",
    "# 데이터로더 생성\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True, drop_last = True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=True, drop_last = True)\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False, drop_last = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import weight_norm\n",
    "import math\n",
    "\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        # Compute the positional encodings once in log space.\n",
    "        pe = torch.zeros(max_len, d_model).float()\n",
    "        pe.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, max_len).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "class TokenEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(TokenEmbedding, self).__init__()\n",
    "        padding = 1 if torch.__version__ >= '1.5.0' else 2\n",
    "        self.tokenConv = nn.Conv1d(in_channels=c_in, out_channels=d_model,\n",
    "                                   kernel_size=3, padding=padding, padding_mode='circular', bias=False)\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv1d):\n",
    "                nn.init.kaiming_normal_(\n",
    "                    m.weight, mode='fan_in', nonlinearity='leaky_relu')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.tokenConv(x.permute(0, 2, 1)).transpose(1, 2)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FixedEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model):\n",
    "        super(FixedEmbedding, self).__init__()\n",
    "\n",
    "        w = torch.zeros(c_in, d_model).float()\n",
    "        w.require_grad = False\n",
    "\n",
    "        position = torch.arange(0, c_in).float().unsqueeze(1)\n",
    "        div_term = (torch.arange(0, d_model, 2).float()\n",
    "                    * -(math.log(10000.0) / d_model)).exp()\n",
    "\n",
    "        w[:, 0::2] = torch.sin(position * div_term)\n",
    "        w[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.emb = nn.Embedding(c_in, d_model)\n",
    "        self.emb.weight = nn.Parameter(w, requires_grad=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.emb(x).detach()\n",
    "\n",
    "class TemporalEmbedding(nn.Module):\n",
    "    def __init__(self, d_model, embed_type='fixed', freq='d'):\n",
    "        super(TemporalEmbedding, self).__init__()\n",
    "\n",
    "        minute_size = 4\n",
    "        hour_size = 24\n",
    "        weekday_size = 7\n",
    "        day_size = 32\n",
    "        month_size = 13\n",
    "        year_size = 6\n",
    "\n",
    "        Embed = FixedEmbedding if embed_type == 'fixed' else nn.Embedding\n",
    "        self.weekday_embed = Embed(weekday_size, d_model)\n",
    "        self.day_embed = Embed(day_size, d_model)\n",
    "        self.month_embed = Embed(month_size, d_model)\n",
    "        # self.year_embed = Embed(year_size, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.long()\n",
    "        weekday_x = self.weekday_embed(x[:, :, 2])\n",
    "        day_x = self.day_embed(x[:, :, 1])\n",
    "        month_x = self.month_embed(x[:, :, 0])\n",
    "        # year_x = self.year_embed(x[:, :, 0])\n",
    "\n",
    "        # return year_x + weekday_x + day_x + month_x \n",
    "        return weekday_x + day_x + month_x \n",
    "\n",
    "\n",
    "class DataEmbedding(nn.Module):\n",
    "    def __init__(self, c_in, d_model, embed_type='fixed', freq='h', dropout=0.1):\n",
    "        super(DataEmbedding, self).__init__()\n",
    "\n",
    "        self.value_embedding = TokenEmbedding(c_in=c_in, d_model=d_model)\n",
    "        self.position_embedding = PositionalEmbedding(d_model=d_model)\n",
    "        self.temporal_embedding = TemporalEmbedding(d_model=d_model, embed_type=embed_type,\n",
    "                                                    freq=freq) \n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, x_mark):\n",
    "        if x_mark is None:\n",
    "            x = self.value_embedding(x) + self.position_embedding(x)\n",
    "        else:\n",
    "            x = self.value_embedding(\n",
    "                x) + self.temporal_embedding(x_mark) + self.position_embedding(x)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "def FFT_for_Period(x, k=5):\n",
    "    # [B, T, C]\n",
    "    xf = torch.fft.rfft(x, dim=1)\n",
    "    # find period by amplitudes\n",
    "    frequency_list = abs(xf).mean(0).mean(-1)\n",
    "    frequency_list[0] = 0\n",
    "    _, top_list = torch.topk(frequency_list, k)\n",
    "    top_list = top_list.detach().cpu().numpy()\n",
    "    period = x.shape[1] // top_list\n",
    "    return period, abs(xf).mean(-1)[:, top_list]\n",
    "\n",
    "\n",
    "class TimesBlock(nn.Module):\n",
    "    def __init__(self, configs):\n",
    "        super(TimesBlock, self).__init__()\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.k = configs.top_k\n",
    "        # parameter-efficient design\n",
    "        self.conv = nn.Sequential(\n",
    "            Inception_Block_V1(configs.d_model, configs.d_ff,\n",
    "                               num_kernels=configs.num_kernels),\n",
    "            nn.GELU(),\n",
    "            Inception_Block_V1(configs.d_ff, configs.d_model,\n",
    "                               num_kernels=configs.num_kernels)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, N = x.size()\n",
    "        period_list, period_weight = FFT_for_Period(x, self.k)\n",
    "\n",
    "        res = []\n",
    "        for i in range(self.k):\n",
    "            period = period_list[i]\n",
    "            # padding\n",
    "            if (self.seq_len + self.pred_len) % period != 0:\n",
    "                length = (\n",
    "                                 ((self.seq_len + self.pred_len) // period) + 1) * period\n",
    "                padding = torch.zeros([x.shape[0], (length - (self.seq_len + self.pred_len)), x.shape[2]]).to(x.device)\n",
    "                out = torch.cat([x, padding], dim=1)\n",
    "            else:\n",
    "                length = (self.seq_len + self.pred_len)\n",
    "                out = x\n",
    "            # reshape\n",
    "            out = out.reshape(B, length // period, period,\n",
    "                              N).permute(0, 3, 1, 2).contiguous()\n",
    "            # 2D conv: from 1d Variation to 2d Variation\n",
    "            out = self.conv(out)\n",
    "            # reshape back\n",
    "            out = out.permute(0, 2, 3, 1).reshape(B, -1, N)\n",
    "            res.append(out[:, :(self.seq_len + self.pred_len), :])\n",
    "        res = torch.stack(res, dim=-1)\n",
    "        # adaptive aggregation\n",
    "        period_weight = F.softmax(period_weight, dim=1)\n",
    "        period_weight = period_weight.unsqueeze(\n",
    "            1).unsqueeze(1).repeat(1, T, N, 1)\n",
    "        res = torch.sum(res * period_weight, -1)\n",
    "        # residual connection\n",
    "        res = res + x\n",
    "        return res\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, configs):\n",
    "        super(Model, self).__init__()\n",
    "        self.configs = configs\n",
    "        self.task_name = configs.task_name\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.label_len = configs.label_len\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.model = nn.ModuleList([TimesBlock(configs)\n",
    "                                    for _ in range(configs.e_layers)])\n",
    "        self.enc_embedding = DataEmbedding(configs.enc_in, configs.d_model, configs.embed, configs.freq,\n",
    "                                           configs.dropout)\n",
    "        self.layer = configs.e_layers\n",
    "        self.layer_norm = nn.LayerNorm(configs.d_model)\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.predict_linear = nn.Linear(\n",
    "                self.seq_len, self.pred_len + self.seq_len)\n",
    "            self.projection = nn.Linear(\n",
    "                configs.d_model, configs.c_out, bias=True)\n",
    "\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc):\n",
    "        # Normalization from Non-stationary Transformer\n",
    "        means = x_enc.mean(1, keepdim=True).detach()\n",
    "        x_enc = x_enc - means\n",
    "        stdev = torch.sqrt(\n",
    "            torch.var(x_enc, dim=1, keepdim=True, unbiased=False) + 1e-5)\n",
    "        x_enc /= stdev\n",
    "\n",
    "        # embedding\n",
    "        enc_out = self.enc_embedding(x_enc, x_mark_enc)  # [B,T,C]\n",
    "        enc_out = self.predict_linear(enc_out.permute(0, 2, 1)).permute(\n",
    "            0, 2, 1)  # align temporal dimension\n",
    "        # TimesNet\n",
    "        for i in range(self.layer):\n",
    "            enc_out = self.layer_norm(self.model[i](enc_out))\n",
    "        # porject back\n",
    "        dec_out = self.projection(enc_out)\n",
    "\n",
    "        # De-Normalization from Non-stationary Transformer\n",
    "        dec_out = dec_out * \\\n",
    "                  (stdev[:, 0, :].unsqueeze(1).repeat(\n",
    "                      1, self.pred_len + self.seq_len, 1))\n",
    "        dec_out = dec_out + \\\n",
    "                  (means[:, 0, :].unsqueeze(1).repeat(\n",
    "                      1, self.pred_len + self.seq_len, 1))\n",
    "        return dec_out\n",
    "\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc)\n",
    "            return dec_out[:, -self.pred_len:, :]  # [B, L, D]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Configs:\n",
    "    def __init__(self):\n",
    "        self.seq_len = 64\n",
    "        self.label_len = 28\n",
    "        self.pred_len = 28\n",
    "        self.top_k = 5\n",
    "        self.d_model = 32\n",
    "        self.d_ff = 32\n",
    "        self.num_kernels = 3\n",
    "        self.e_layers = 2\n",
    "        self.d_layers = 1\n",
    "        self.enc_in = 1\n",
    "        self.dec_in = 1\n",
    "        self.c_out = 1\n",
    "        self.embed = 16\n",
    "        self.freq = 'd'\n",
    "        self.dropout = 0.1\n",
    "        self.task_name = 'short_term_forecast'\n",
    "        self.c_out = 1\n",
    "        self.seasonal_patternes = 'Monthly'\n",
    "        self.features = 'S'\n",
    "\n",
    "configs = Configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=7, verbose=False, delta=0):\n",
    "        self.patience = patience # how many times will you tolerate for loss not being on decrease\n",
    "        self.verbose = verbose  # whether to print tip info\n",
    "        self.counter = 0 # now how many times loss not on decrease\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "\n",
    "    def __call__(self, val_loss, model, path):\n",
    "        score = -val_loss\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "\n",
    "        # meaning: current score is not 'delta' better than best_score, representing that \n",
    "        # further training may not bring remarkable improvement in loss. \n",
    "        elif score < self.best_score + self.delta:  \n",
    "            self.counter += 1\n",
    "            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            # 'No Improvement' times become higher than patience --> Stop Further Training\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "\n",
    "        else: #model's loss is still on decrease, save the now best model and go on training\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model, path)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model, path):\n",
    "    ### used for saving the current best model\n",
    "        if self.verbose:\n",
    "            print(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), path + '/' + 'checkpoint.pth')\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vali(model, vali_loader, criterion):\n",
    "        total_loss = []\n",
    "\n",
    "        #evaluation mode\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark,_,_) in enumerate(vali_loader):\n",
    "                batch_x = batch_x.float().to(device)\n",
    "                batch_y = batch_y.float()\n",
    "\n",
    "                batch_x_mark = batch_x_mark.float().to(device)\n",
    "                batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "                \n",
    "                outputs = model(batch_x, batch_x_mark)\n",
    "                f_dim = 0\n",
    "                outputs = outputs[:, -pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -pred_len:, f_dim:].to(device)\n",
    "\n",
    "                pred = outputs.detach().cpu()\n",
    "                true = batch_y.detach().cpu()\n",
    "\n",
    "                loss = criterion(pred, true)\n",
    "\n",
    "                total_loss.append(loss)\n",
    "        total_loss = np.average(total_loss)\n",
    "        model.train()\n",
    "        return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 cost time: 6.630518198013306\n",
      "Epoch: 1, Steps: 78 | Train Loss: 0.9745246 Vali Loss: 0.2974105 Test Loss: 0.4719639\n",
      "Validation loss decreased (inf --> 0.297411).  Saving model ...\n",
      "Epoch: 2 cost time: 5.505518913269043\n",
      "Epoch: 2, Steps: 78 | Train Loss: 0.7376696 Vali Loss: 0.1657926 Test Loss: 0.3786559\n",
      "Validation loss decreased (0.297411 --> 0.165793).  Saving model ...\n",
      "Epoch: 3 cost time: 5.489583730697632\n",
      "Epoch: 3, Steps: 78 | Train Loss: 0.7003860 Vali Loss: 0.1192132 Test Loss: 0.3251331\n",
      "Validation loss decreased (0.165793 --> 0.119213).  Saving model ...\n",
      "Epoch: 4 cost time: 5.642007112503052\n",
      "Epoch: 4, Steps: 78 | Train Loss: 0.6836522 Vali Loss: 0.1676301 Test Loss: 0.3283365\n",
      "EarlyStopping counter: 1 out of 3\n",
      "Epoch: 5 cost time: 5.747663259506226\n",
      "Epoch: 5, Steps: 78 | Train Loss: 0.6743232 Vali Loss: 0.1284957 Test Loss: 0.3189631\n",
      "EarlyStopping counter: 2 out of 3\n",
      "Epoch: 6 cost time: 5.85167384147644\n",
      "Epoch: 6, Steps: 78 | Train Loss: 0.6524324 Vali Loss: 0.1199608 Test Loss: 0.3289837\n",
      "EarlyStopping counter: 3 out of 3\n",
      "Early stopping\n",
      "torch.Size([1, 28, 1])\n",
      "A\n",
      "test shape: (1, 1, 28, 1) (1, 1, 28, 1)\n",
      "test shape: (1, 28, 1) (1, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "model = Model(configs)\n",
    "epochs = 10\n",
    "learning_rate = 0.05\n",
    "\n",
    "seq_len = 64\n",
    "pred_len = 28\n",
    "label_len = 28\n",
    "################\n",
    "\n",
    "\n",
    "train_loader = dataloader_train\n",
    "vali_loader = dataloader_val\n",
    "test_loader = dataloader_test\n",
    "\n",
    "_, _, _, _, test_mean, test_std = next(iter(dataloader_test))\n",
    "\n",
    "\n",
    "# set path of checkpoint for saving and loading model\n",
    "path = 'Savemodel'\n",
    "time_now = time.time()\n",
    "\n",
    "train_steps = len(train_loader)\n",
    "\n",
    "# EarlyStopping is typically a custom class or function that monitors the performance \n",
    "# of a model during training, usually by tracking a certain metric (commonly validation \n",
    "# loss or accuracy).It's a common technique used in deep learning to prevent overfitting \n",
    "# during the training\n",
    "early_stopping = EarlyStopping(patience=3, verbose=True)\n",
    "\n",
    "#Optimizer and Loss Function Selection\n",
    "model_optim = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    iter_count = 0\n",
    "    train_loss = []\n",
    "    model.train()\n",
    "    epoch_time = time.time()\n",
    "\n",
    "    #begin training in this epoch\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark,_,_) in enumerate(train_loader):\n",
    "        iter_count += 1\n",
    "        model_optim.zero_grad()\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        batch_x = batch_x.float().to(device)  #input features\n",
    "        batch_y = batch_y.float().to(device)  #target features\n",
    "\n",
    "        # _mark holds information about time-related features. Specifically, it is a \n",
    "        # tensor that encodes temporal information and is associated with the \n",
    "        # input data batch_x.\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "        \n",
    "        outputs = model(batch_x, batch_x_mark)\n",
    "        f_dim = 0 \n",
    "        # f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -pred_len:, f_dim:]\n",
    "        batch_y = batch_y[:, -pred_len:, f_dim:].to(device)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        train_loss.append(loss.item())\n",
    "\n",
    "        # When train rounds attain some 100-multiple, print speed, left time, loss. etc feedback\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(\"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item()))\n",
    "            speed = (time.time() - time_now) / iter_count\n",
    "            left_time = speed * ((epochs - epoch) * train_steps - i)\n",
    "            print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "            iter_count = 0\n",
    "            time_now = time.time()\n",
    "\n",
    "        # #BP\n",
    "        # if args.use_amp:\n",
    "        #     scaler.scale(loss).backward()\n",
    "        #     scaler.step(model_optim)\n",
    "        #     scaler.update()\n",
    "        # else:\n",
    "        #     loss.backward()\n",
    "        #     model_optim.step()\n",
    "        loss.backward()\n",
    "        model_optim.step()\n",
    "\n",
    "    \n",
    "    #This epoch comes to end, print information\n",
    "    print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "    train_loss = np.average(train_loss)\n",
    "\n",
    "    #run test and validation on current model\n",
    "    vali_loss = vali(model, vali_loader, criterion)\n",
    "    test_loss = vali(model, test_loader, criterion)\n",
    "\n",
    "    #print train, test, vali loss information\n",
    "    print(\"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "        epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "    \n",
    "    #Decide whether to trigger Early Stopping. if early_stop is true, it means that \n",
    "    #this epoch's training is now at a flat slope, so stop further training for this epoch.\n",
    "    early_stopping(vali_loss, model, path)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping\")\n",
    "        break\n",
    "\n",
    "    # #adjust learning keys\n",
    "    # adjust_learning_rate(model_optim, epoch + 1, args)\n",
    "\n",
    "# best_model_path = path + '/' + 'checkpoint.pth'\n",
    "\n",
    "# # loading the trained model's state dictionary from a saved checkpoint file \n",
    "# # located at best_model_path.\n",
    "# model.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "preds = []\n",
    "trues = []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i, (batch_x, batch_y, batch_x_mark, batch_y_mark,_,_) in enumerate(test_loader):\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        batch_x = batch_x.float().to(device)\n",
    "        batch_y = batch_y.float().to(device)\n",
    "\n",
    "        batch_x_mark = batch_x_mark.float().to(device)\n",
    "        batch_y_mark = batch_y_mark.float().to(device)\n",
    "\n",
    "        \n",
    "        outputs = model(batch_x, batch_x_mark)\n",
    "        print(outputs.shape)\n",
    "        print('A')\n",
    "\n",
    "        f_dim = 0\n",
    "        outputs = outputs[:,-pred_len:, f_dim:]\n",
    "        batch_y = batch_y[:,-pred_len:, f_dim:].to(device)\n",
    "    \n",
    "        outputs = outputs.detach().cpu().numpy()\n",
    "        batch_y = batch_y.detach().cpu().numpy()\n",
    "\n",
    "        #inverse the data if scaled\n",
    "        \n",
    "        outputs = outputs * test_std[0].item() + test_mean[0].item()\n",
    "        batch_y = batch_y * test_std[0].item() + test_mean[0].item()\n",
    "  \n",
    "        pred = outputs#.view(-1).numpy()\n",
    "        true = batch_y\n",
    "\n",
    "        preds.append(pred)\n",
    "        trues.append(true)\n",
    "\n",
    "       \n",
    "preds = np.array(preds)\n",
    "trues = np.array(trues)  # shape[batch_num, batch_size, pred_len, features]\n",
    "print('test shape:', preds.shape, trues.shape)\n",
    "preds = preds.reshape(-1, preds.shape[-2], preds.shape[-1])\n",
    "trues = trues.reshape(-1, trues.shape[-2], trues.shape[-1])\n",
    "print('test shape:', preds.shape, trues.shape)\n",
    "    \n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[1506.7806  ]\n",
      "  [-491.20166 ]\n",
      "  [1751.3953  ]\n",
      "  [1651.3871  ]\n",
      "  [2577.1685  ]\n",
      "  [2423.4749  ]\n",
      "  [2509.41    ]\n",
      "  [1809.4152  ]\n",
      "  [  72.426025]\n",
      "  [2460.4446  ]\n",
      "  [2623.8328  ]\n",
      "  [3206.3164  ]\n",
      "  [3100.8818  ]\n",
      "  [2975.5361  ]\n",
      "  [2771.7603  ]\n",
      "  [  96.093506]\n",
      "  [3603.1216  ]\n",
      "  [3434.037   ]\n",
      "  [3576.2192  ]\n",
      "  [3613.2058  ]\n",
      "  [3407.312   ]\n",
      "  [3305.0933  ]\n",
      "  [ 660.0215  ]\n",
      "  [3636.4763  ]\n",
      "  [3040.2854  ]\n",
      "  [3773.808   ]\n",
      "  [3439.8186  ]\n",
      "  [3375.7178  ]]]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
