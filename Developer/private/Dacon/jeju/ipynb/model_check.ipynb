{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyeongyukang/Library/Python/3.9/lib/python/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20231101, 20231102, 20231103, 20231104, 20231105]\n"
     ]
    }
   ],
   "source": [
    "## \n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "##\n",
    "from DLinear import LTSF_DLinear\n",
    "from DLinear import moving_avg\n",
    "from utils import standardization\n",
    "from utils import time_slide_df\n",
    "from utils import Data\n",
    "from utils import data_split\n",
    "from utils import date_range_to_numeric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load dataset and split by item, corporation, location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_loading\n",
    "data = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "\n",
    "\n",
    "data_list = data_split(data)\n",
    "test_list = data_split(test)\n",
    "\n",
    "unique_code = ['BC_C_J', 'TG_B_J', 'CR_B_J', 'RD_E_S', 'BC_A_J', 'CB_F_J', 'RD_D_J', 'TG_A_S', 'BC_E_S', 'CR_D_J', 'BC_A_S', 'BC_B_S', 'TG_E_J', \n",
    "               'CR_E_S', 'RD_F_J', 'BC_E_J', 'TG_A_J', 'CR_C_J', 'CR_D_S', 'TG_C_J', 'CB_A_S', 'TG_D_J', 'CR_E_J', 'RD_C_S', 'BC_C_S', 'CB_E_J', \n",
    "               'RD_E_J', 'BC_D_J', 'CR_A_J', 'TG_E_S', 'TG_C_S', 'TG_D_S', 'RD_A_S', 'RD_A_J', 'RD_D_S', 'TG_B_S', 'CB_D_J', 'CB_A_J', 'BC_B_J']\n",
    "\n",
    "## CR_D_S, CB_A_S, BC_B_S, BC_C_S, CR_E_S, RD_C_S : 3월에 일반적으로 0을 가짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1523\n"
     ]
    }
   ],
   "source": [
    "## prepare dataset for training\n",
    "dataset_code = unique_code[1]\n",
    "train_df = data_list[f'data_{dataset_code}'].reset_index(drop=True)\n",
    "test_df = test_list[f'data_{dataset_code}'].reset_index(drop=True)\n",
    "print(len(train_df))\n",
    "\n",
    "# preprocess timestamp\n",
    "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## paramaters\n",
    "window_size = 7\n",
    "forcast_size= 1\n",
    "batch_size = 32\n",
    "targets = 'price(원/kg)'\n",
    "date = 'timestamp'\n",
    "not_col = 'timestamp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_slide_df1(df, window_size, forcast_size, date, target):\n",
    "    df_ = df.copy()\n",
    "    data_list = []\n",
    "    dap_list = []\n",
    "    date_list = []\n",
    "    for idx in range(0, df_.shape[0]-window_size-forcast_size+1): # range(0,1516)\n",
    "        x = df_.loc[idx:idx+window_size-1, target].values.reshape(window_size, 1)\n",
    "        y = df_.loc[idx+window_size:idx+window_size+forcast_size-1, target].values\n",
    "        date_ = df_.loc[idx+window_size:idx+window_size+forcast_size-1, date].values\n",
    "        data_list.append(x)\n",
    "        dap_list.append(y)\n",
    "        date_list.append(date_)\n",
    "    return np.array(data_list, dtype='float32'), np.array(dap_list, dtype='float32'), np.array(date_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, train_date = time_slide_df1(train_df, window_size, forcast_size, date, targets)\n",
    "# train_x.shape : (1486, 31, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.412e+03, 0.000e+00, 3.540e+03, 3.141e+03, 6.382e+03, 3.558e+03,\n",
       "       3.470e+03, 1.000e+00])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y[-7:].reshape(1,-1)\n",
    "np.append(train_y[-7:].reshape(1,-1),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.Size([32, 7, 1])\n",
    "ouput = torch.Size([1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "valid_y[-7:]\n",
    "all = [1,2,3,4,5,6,7]\n",
    "pred = []\n",
    "torch.Tensor(all)\n",
    "model(all)-> output1(3월 4일 결과 출력)\n",
    "\n",
    "all.append(output1.item())\n",
    "pred.append(output1.item())\n",
    "\n",
    "all = [1,2,3,4,5,6,7,output1]\n",
    "all.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_y[-7:].reshape(1,-1)\n",
    "pred = []\n",
    "for i in range(28):\n",
    "    pred_input = torch.Tensor(data).reshape(1,7,1)\n",
    "    pred_output = model(pred_input)\n",
    "\n",
    "    np.append(data,pred_output.item())\n",
    "    pred.append(pred_output.item())\n",
    "    data.pop(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_function_and_slide(data, window_size):\n",
    "    input = []  # 결과 값을 저장할 리스트\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        window = data[i:i+window_size]  # 현재 윈도우\n",
    "        if len(window) == window_size:\n",
    "            # 윈도우 크기가 7인 경우 함수를 적용하고 결과를 결과 리스트에 추가\n",
    "            output = model(window)\n",
    "            input.append(output)\n",
    "\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "pred = {}\n",
    "\n",
    "# for i in tqdm(range(len(unique_code))):\n",
    "for i in tqdm(range(len(unique_code))):\n",
    "\n",
    "    ## prepare dataset for training\n",
    "    dataset_code = unique_code[i]\n",
    "\n",
    "    train_df = data_list[f'data_{dataset_code}'].reset_index(drop=True)\n",
    "    test_df = test_list[f'data_{dataset_code}'].reset_index(drop=True)\n",
    "    print(len(train_df))\n",
    "\n",
    "    # train_df = train_df.drop(['ID', 'item', 'corporation', 'location'], axis=1)\n",
    "    # test_df = test_df.drop(['ID', 'item', 'corporation', 'location'], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    train_df['timestamp'] = pd.to_datetime(train_df['timestamp'])\n",
    "    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'])\n",
    "    print(\"Dataset prepared\")\n",
    "    print()\n",
    "\n",
    "    ## paramaters\n",
    "    window_size = 31\n",
    "    forcast_size= 7\n",
    "    batch_size = 32\n",
    "    targets = 'price(원/kg)'\n",
    "    date = 'timestamp'\n",
    "    not_col = 'timestamp'\n",
    "\n",
    "    print(\"Start preprocessing\")\n",
    "    print()\n",
    "    # train_df_fe, test_df_fe, mean_, std_ = standardization(train_df, test_df, not_col, targets)\n",
    "    # train_x, train_y, train_date = time_slide_df(train_df_fe, window_size, forcast_size, date, targets)\n",
    "    # test_x, test_y, test_date = time_slide_df(test_df_fe, window_size, forcast_size, date, targets)\n",
    "\n",
    "    train_x, train_y, train_date = time_slide_df(train_df, window_size, forcast_size, date, targets)\n",
    "    # test_x, test_y, test_date = time_slide_df(test_df, window_size, forcast_size, date, targets)\n",
    "\n",
    "\n",
    "    train_ds = Data(train_x[:1400], train_y[:1400])\n",
    "    valid_ds = Data(train_x[1400:], train_y[1400:])\n",
    "    # test_ds = Data(test_x, test_y)\n",
    "\n",
    "    train_dl = DataLoader(train_ds, batch_size = batch_size, shuffle=True,)\n",
    "    valid_dl = DataLoader(valid_ds, batch_size = train_x[1200:].shape[0], shuffle=False)\n",
    "    # test_dl  = DataLoader(test_ds,  batch_size = test_x.shape[0], shuffle=False)\n",
    "    print(\"Success preprocessing\")\n",
    "\n",
    "\n",
    "    print(\"Paramater for training\")\n",
    "    print()\n",
    "    train_loss_list = []\n",
    "    valid_loss_list = []\n",
    "    test_loss_list = []\n",
    "    epochs = 100\n",
    "    lr = 0.001\n",
    "    DLinear_model = LTSF_DLinear(\n",
    "                                window_size=window_size,\n",
    "                                forcast_size=forcast_size,\n",
    "                                kernel_size=25,\n",
    "                                individual=False,\n",
    "                                feature_size=1,\n",
    "                                )\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(DLinear_model.parameters(), lr=lr)\n",
    "    max_loss = 999999999\n",
    "\n",
    "    print(\"strat training\")\n",
    "    print()\n",
    "\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        loss_list = []\n",
    "        DLinear_model.train()\n",
    "        for batch_eeeeidx, (data, target) in enumerate(train_dl):\n",
    "            optimizer.zero_grad()\n",
    "            output = DLinear_model(data)\n",
    "            loss = criterion(output, target.unsqueeze(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(np.sqrt(loss.item()))  \n",
    "        train_loss_list.append(np.sqrt(np.mean(loss_list)))\n",
    "\n",
    "        DLinear_model.eval()\n",
    "        with torch.no_grad():\n",
    "            for data, target in valid_dl:\n",
    "                output = DLinear_model(data)\n",
    "                # if epoch == 100:\n",
    "                #     temp = output.reshape(1,-1)\n",
    "                #     temp1 = temp[:,-61:].reshape(1,61,1)\n",
    "                #     pred[dataset_code] = DLinear_model(temp1).reshape(1,-1).numpy()\n",
    "                valid_loss = np.sqrt(criterion(output, target.unsqueeze(-1)).item())\n",
    "                valid_loss_list.append(np.sqrt(valid_loss.item()))\n",
    "            \n",
    "            # for data, target in test_dl:\n",
    "            #     output = DLinear_model(data)\n",
    "            #     test_loss = criterion(output, target.unsqueeze(-1))\n",
    "            #     test_loss_list.append(test_loss)\n",
    "\n",
    "        # if valid_loss < max_loss:\n",
    "        #     torch.save(DLinear_model, f'DLinear_model_{dataset_code}.pth')\n",
    "        #     max_loss = valid_loss\n",
    "        #     # print(\"valid_loss={:.3f}, test_los{:.3f}, Model Save\".format(valid_loss, test_loss))\n",
    "        #     print(\"valid_loss={:.3f}, Model Save\".format(valid_loss))\n",
    "        #     dlinear_best_epoch = epoch\n",
    "        #     dlinear_best_train_loss = np.mean(loss_list)\n",
    "        #     dlinear_best_valid_loss = np.mean(valid_loss.item())\n",
    "\n",
    "        # print(\"epoch = {}, train_loss : {:.3f}, valid_loss : {:.3f}, test_loss : {:.3f}\".format(epoch, np.mean(loss_list), valid_loss, test_loss))\n",
    "        print(\"epoch = {}, train_loss : {:.3f}, valid_loss : {:.3f}\".format(epoch, np.mean(loss_list), valid_loss))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## Storing result\n",
    "\n",
    "# date_col = []\n",
    "# date = 20230304\n",
    "# for i in range(28):\n",
    "#     date_col.append(date + i)\n",
    "# print(date_col)\n",
    "\n",
    "\n",
    "\n",
    "# result = []\n",
    "# for i in range(len(unique_code)):\n",
    "#     pred_adj = pred[unique_code[i]][0][3:]\n",
    "#     for j in range(28):\n",
    "#         code = unique_code[i]\n",
    "#         final = pred_adj[j]\n",
    "#         if date_col[j] in [20230305, 20230312, 20230319, 20230326]:\n",
    "#             final = 0\n",
    "#         else:\n",
    "#             final = final\n",
    "\n",
    "#         code_time = f\"{unique_code[i]}_{date_col[j]}\"\n",
    "#         result.append([code_time, final])\n",
    "    \n",
    "\n",
    "# result_df = pd.DataFrame({'ID': result[:, 0], 'pred': result[:, 1]})\n",
    "# submission = pd.read_csv(\"data/sample_submission.csv\")\n",
    "\n",
    "\n",
    "# # 'submission' 데이터프레임과 'result_df' 데이터프레임을 'ID'를 기준으로 병합\n",
    "# final_submission = submission.merge(result_df, on='ID', how='left')\n",
    "\n",
    "# # 'pred' 값을 'answer' 열에 복사\n",
    "# final_submission['answer'] = final_submission['pred']\n",
    "\n",
    "# # 'pred' 열 삭제\n",
    "# final_submission.drop('pred', axis=1, inplace=True)\n",
    "\n",
    "# # 결과 데이터프레임 출력\n",
    "# print(final_submission.head())\n",
    "\n",
    "\n",
    "# # 업데이트된 데이터프레임을 새로운 CSV 파일로 저장\n",
    "# final_submission.to_csv('submission_vanila.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
